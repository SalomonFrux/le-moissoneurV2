<!-- Generated by SpecStory -->

# Scraper data population issue in scraperService.js (2025-05-26 00:35:56)

_**User**_

Can you compare this code below and the code at  backend\src\services\scraperService.js and tell me or show me the difference? 

The problem I am facing: With the code below, my scraper_data table was being populated but after some update the code at  backend\src\services\scraperService.js   does not populate the columns:

these culumns are no longer populated(  lien TEXT,
    secteur TEXT,
    pays TEXT,
    source TEXT,
    site_web TEXT,
    email TEXT,
    telephone TEXT,
    adresse TEXT,)

YOu might want to read my codebase or the file at backend\src\scrapers\playwrightScraper.js too. 

Old working code: scraperService.js
const puppeteer = require('puppeteer-extra');
const StealthPlugin = require('puppeteer-extra-plugin-stealth');
puppeteer.use(StealthPlugin());
const logger = require('../utils/logger');
const { supabase } = require('../db/supabase');
const { genericScraper } = require('../scrapers/genericScraper');
const { newsPortalScraper } = require('../scrapers/newsPortalScraper');
const { playwrightScraper } = require('../scrapers/playwrightScraper');

// Configuration
const MAX_RETRIES = 3;
const INITIAL_DELAY = 3000;
const MAX_DELAY = 30000;
const MAX_CONCURRENT_PAGES = 2;

let lastRequestTime = 0;
const activeRequests = new Set();

async function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

async function waitForRateLimit() {
  const now = Date.now();
  const timeSinceLastRequest = now - lastRequestTime;
  
  if (timeSinceLastRequest < INITIAL_DELAY) {
    await sleep(INITIAL_DELAY - timeSinceLastRequest);
  }
  
  lastRequestTime = Date.now();
}

async function withRetry(operation, maxRetries = MAX_RETRIES) {
  let lastError;
  
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      // Rate limiting
      await waitForRateLimit();
      
      return await operation();
    } catch (error) {
      lastError = error;
      
      const isRetryableError = 
        error.message.includes('socket hang up') ||
        error.message.includes('net::') ||
        error.message.includes('Protocol error') ||
        error.message.includes('Target closed') ||
        error.message.includes('Connection closed') ||
        error.message.includes('ERR_CONNECTION_RESET') ||
        error.message.includes('ERR_CONNECTION_REFUSED') ||
        error.message.includes('ERR_EMPTY_RESPONSE');
      
      if (!isRetryableError) throw error;
      
      if (attempt < maxRetries) {
        const delay = Math.min(
          INITIAL_DELAY * Math.pow(2, attempt - 1) + Math.random() * 1000,
          MAX_DELAY
        );
        
        logger.warn(`Attempt ${attempt} failed, retrying in ${delay}ms: ${error.message}`);
        await sleep(delay);
      }
    }
  }
  
  throw lastError;
}

async function createBrowserInstance() {
  const args = [
    '--no-sandbox',
    '--disable-setuid-sandbox',
    '--disable-dev-shm-usage',
    '--disable-accelerated-2d-canvas',
    '--disable-gpu',
    '--window-size=1920x1080',
    '--no-first-run',
    '--no-zygote',
    '--disable-extensions',
    '--disable-component-extensions-with-background-pages',
    '--disable-background-networking',
    '--disable-sync',
    '--metrics-recording-only',
    '--disable-default-apps',
    '--mute-audio',
    '--no-default-browser-check',
    '--disable-blink-features=AutomationControlled',
    '--disable-http2',
    '--host-resolver-rules="MAP * ~NOTFOUND, EXCLUDE localhost"',
    '--disable-features=site-per-process,TranslateUI',
    '--disable-client-side-phishing-detection',
    '--disable-component-update',
    '--disable-domain-reliability',
    '--disable-breakpad',
    '--disable-ipc-flooding-protection'
  ];

  // Wait for concurrent requests limit
  while (activeRequests.size >= MAX_CONCURRENT_PAGES) {
    await sleep(1000);
  }

  try {
    const browser = await puppeteer.launch({
      headless: "new",
      args,
      ignoreHTTPSErrors: true,
      defaultViewport: {
        width: 1920,
        height: 1080,
        deviceScaleFactor: 1,
        isMobile: false,
        hasTouch: false
      },
      timeout: 60000,
      protocolTimeout: 60000
    });

    browser.on('disconnected', () => {
      activeRequests.delete(browser);
    });

    activeRequests.add(browser);
    return browser;
  } catch (error) {
    logger.error(`Failed to create browser instance: ${error.message}`);
    throw error;
  }
}

async function executeScraper(scraper) {
  logger.info(`Starting scraper: ${scraper.name} (${scraper.id})`);
  
  let browser;
  let scrapedData = [];
  
  // Helper function to extract data using regex patterns
  const extractInfo = (text, pattern) => {
    if (!text) return null;
    const match = text.match(new RegExp(pattern, 'i'));
    return match ? match[1].trim() : null;
  };

  try {
    await withRetry(async () => {
      // Use Playwright if type is 'playwright'
      if (scraper.type === 'playwright') {
        // Get existing data before scraping
        const { data: existingData } = await supabase
          .from('scraped_data')
          .select('*')
          .eq('scraper_id', scraper.id);

        // Scrape new data
        scrapedData = await playwrightScraper(
          scraper.source || scraper.url,
          {
            main: scraper.selectors?.main,
            child: scraper.selectors?.child || {},
            pagination: scraper.selectors?.pagination,
            dropdownClick: scraper.selectors?.dropdownClick
          }
        );
        
        // Map Playwright results to expected format, preserving existing values
        scrapedData = scrapedData.map((result, index) => {
          const existing = existingData?.[index] || {};
          const existingMetadata = existing.metadata || {};
          
          // Only update fields that have new selectors
          const newMetadata = {
            name: result.name || existingMetadata.name,
            phone: result.phone || existingMetadata.phone,
            email: result.email || existingMetadata.email,
            website: result.website || existingMetadata.website,
            address: result.address || existingMetadata.address,
            category: result.category || existingMetadata.category
          };

          return {
            title: newMetadata.name || scraper.name,
            content: result.text,
            url: scraper.source || scraper.url,
            metadata: newMetadata
          };
        });
        return;
      }
      browser = await createBrowserInstance();
      const scrapeOperation = async () => {
        if (scraper.type === 'news') {
          return await newsPortalScraper(browser, scraper);
        } else {
          return await genericScraper(browser, scraper);
        }
      };
      scrapedData = await scrapeOperation();
    });

    if (scrapedData.length > 0) {
      const dataToInsert = scrapedData.map(item => {
        // Extract data from metadata and content
        const metadata = item.metadata || {};
        const content = item.content || '';
        
        // Use metadata values if available, otherwise try to extract from content
        const extractedData = {
          name: metadata.name || item.title || 'Unknown',
          sector: metadata.category || 'Unknown',
          country: scraper.country || 'Unknown',
          website: metadata.website || extractInfo(content, /Site web\s*:\s*([^\n]+)/i),
          email: metadata.email || extractInfo(content, /E-mail\s*:\s*([^\n]+)/i),
          phone: metadata.phone || extractInfo(content, /Tel\s*:\s*([^\n]+)/i),
          address: metadata.address || extractInfo(content, /Address\s*:\s*([^\n]+)/i),
          source: scraper.source,
          last_updated: new Date().toISOString()
        };

        // Clean up website URL if it exists and isn't already formatted
        if (extractedData.website && !extractedData.website.toLowerCase().startsWith('http')) {
          extractedData.website = `https://${extractedData.website}`;
        }

        return {
          scraper_id: scraper.id,
          nom: extractedData.name,
          secteur: extractedData.sector || scraper.sector || 'Aucune donnÃ©e',
          pays: scraper.country || 'Maroc',
          site_web: extractedData.website,
          email: extractedData.email,
          telephone: extractedData.phone,
          adresse: extractedData.address,
          contenu: item.content,
          lien: item.url || scraper.source || null,
          metadata: {
            ...metadata,
            original_content: item.content,
            extracted_data: extractedData
          },
          created_at: new Date().toISOString()
        };
      });

      await withRetry(async () => {
        // First, delete existing entries for this scraper
        const { error: deleteError } = await supabase
          .from('scraped_data')
          .delete()
          .eq('scraper_id', scraper.id);

        if (deleteError) {
          throw new Error(`Failed to delete existing data: ${deleteError.message}`);
        }

        // Then insert the new data
        const { error: insertError } = await supabase
          .from('scraped_data')
          .insert(dataToInsert);

        if (insertError) {
          throw new Error(`Failed to store scraped data: ${insertError.message}`);
        }
      });

      // Extract and insert company data
      for (const item of scrapedData) {
        let content = item.content;
        let metadata = item.metadata;

        // Parse content if it's a string
        if (typeof content === 'string') {
          try {
            const parsedContent = JSON.parse(content);
            content = parsedContent;
            metadata = parsedContent.metadata || metadata;
          } catch (e) {
            // If parsing fails, use the string content
            content = { text: content };
          }
        }

        // Extract information from content
        const text = content.text || '';
        
        // Extract data using regex patterns
        const extractedData = {
          name: item.title || 'Unknown',
          sector: 'Unknown',
          country: scraper.country || 'Unknown', // Use the country from scraper config
          website: extractInfo(text, /Site web\s*:\s*([^\n]+)/i),
          email: extractInfo(text, /E-mail\s*:\s*([^\n]+)/i),
          phone: extractInfo(text, /Tel\s*:\s*([^\n]+)/i),
          address: extractInfo(text, /Address\s*:\s*([^\n]+)/i),
          source: scraper.source, // Use the source URL instead of scraper name
          last_updated: new Date().toISOString()
        };

        // Clean up the data
        if (extractedData.website && !extractedData.website.startsWith('http')) {
          extractedData.website = `http://${extractedData.website}`;
        }

        // Remove any HTML tags from the extracted data
        Object.keys(extractedData).forEach(key => {
          if (typeof extractedData[key] === 'string') {
            extractedData[key] = extractedData[key].replace(/<[^>]*>/g, '');
          }
        });

        await supabase.from('companies').upsert(extractedData, { 
          onConflict: ['name', 'country', 'source'],
          ignoreDuplicates: false
        });
      }

      const { data: currentData, error: countError } = await supabase
        .from('scraped_data')
        .select('id')
        .eq('scraper_id', scraper.id);

      if (countError) {
        logger.error(`Error getting data count: ${countError.message}`);
      }

      const dataCount = currentData?.length || 0;

      await withRetry(async () => {
        await supabase
          .from('scrapers')
          .update({ 
            status: 'completed',
            last_run: new Date().toISOString(),
            data_count: dataCount
          })
          .eq('id', scraper.id);
      });

      logger.info(`Stored ${dataToInsert.length} items for scraper ${scraper.id}. Total items: ${dataCount}`);
    } else {
      logger.warn(`No data scraped for ${scraper.id}`);
      
      await withRetry(async () => {
        await supabase
          .from('scrapers')
          .update({ 
            status: 'completed',
            last_run: new Date().toISOString()
          })
          .eq('id', scraper.id);
      });
    }

    return scrapedData;
    
  } catch (error) {
    logger.error(`Error executing scraper ${scraper.id}: ${error.message}`);
    
    await withRetry(async () => {
      await supabase
        .from('scrapers')
        .update({ 
          status: 'error', 
          last_run: new Date().toISOString() 
        })
        .eq('id', scraper.id);
    });
    
    throw error;
  } finally {
    if (browser) {
      try {
        activeRequests.delete(browser);
        await browser.close();
      } catch (error) {
        logger.error(`Error closing browser: ${error.message}`);
      }
    }
  }
}

/**
 * Populate the companies table with data extracted from the scraped_data table.
 */
async function populateCompanies() {
  const { data: scrapedData, error } = await supabase.from('scraped_data').select('*');
  if (error) return logger.error('Error fetching scraped_data:', error);
  for (const record of scrapedData) {
    const meta = record.content || record.metadata || {};
    const company = {
      name: meta.name || record.title || 'Unknown',
      sector: meta.sector || 'Unknown',
      country: meta.country || 'Unknown',
      website: meta.website || null,
      linkedin: meta.linkedin || null,
      email: meta.email || null,
      source: record.scraper_id,
      last_updated: record.scraped_at,
    };
    const { data: exists } = await supabase.from('companies').select('id').eq('name', company.name).single();
    if (!exists) {
      await supabase.from('companies').insert(company);
    }
  }
  logger.info('Companies enrichment done.');
}

// Call after scraping
async function runScraperAndEnrich(scraper) {
  await executeScraper(scraper);
  await populateCompanies();
}

// Helper function to extract city from address
function extractCityFromAddress(address) {
  if (!address) return null;
  const cityMatch = address.match(/\s*-\s*([^-]+)$/);
  return cityMatch ? cityMatch[1].trim() : null;
}

module.exports = {
  executeScraper,
  populateCompanies,
  runScraperAndEnrich
};



old working  at playwright:
const { chromium } = require('playwright');

/**
 * Playwright scraper: supports clicking dropdowns and extracting any content via multiple selectors.
 * @param {string} url - The starting URL.
 * @param {Object} selectors - Object containing main, child, pagination, and dropdownClick selectors
 * @returns {Promise<object[]>}
 */
async function playwrightScraper(url, selectors) {
  const browser = await chromium.launch({ headless: false });
  const page = await browser.newPage();
  let results = [];
  let currentUrl = url;
  let pageNum = 1;

  try {
    while (true) {
      await page.goto(currentUrl, { timeout: 60000 });

      // Click all dropdowns/arrows if a selector is provided
      if (selectors.dropdownClick) {
        const dropdowns = await page.$$(selectors.dropdownClick);
        for (const dropdown of dropdowns) {
          try {
            await dropdown.click();
            await page.waitForTimeout(200);
          } catch (e) {}
        }
      }

      // Wait for the main container to appear
      try {
        await page.waitForSelector(selectors.main, { timeout: 3000 });
      } catch (e) {
        console.error('Main container not found:', e);
        break;
      }

      // Extract data from all matching main containers
      const pageResults = await page.evaluate((config) => {
        const mainContainers = document.querySelectorAll(config.main);
        const results = [];

        mainContainers.forEach(container => {
          const data = {
            text: container.innerText,
            metadata: {}
          };

          // Extract data using child selectors
          if (config.child) {
            Object.entries(config.child).forEach(([key, selector]) => {
              const element = container.querySelector(selector);
              if (element) {
                // Handle different types of elements and attributes
                if (key === 'email' && element.href?.startsWith('mailto:')) {
                  data.metadata[key] = element.href.replace('mailto:', '');
                } else if (key === 'website' && element.href) {
                  data.metadata[key] = element.href;
                } else if (key === 'phone' && element.href?.startsWith('tel:')) {
                  data.metadata[key] = element.href.replace('tel:', '');
                } else {
                  data.metadata[key] = element.innerText.trim();
                }
              }
            });
          }

          results.push(data);
        });

        return results;
      }, selectors);

      results = results.concat(pageResults);

      // Handle pagination if selector is provided
      if (!selectors.pagination) break;
      const nextLink = await page.$(selectors.pagination);
      if (!nextLink) break;

      const href = await nextLink.getAttribute('href');
      if (href) {
        const newUrl = href.startsWith('http') ? href : new URL(href, currentUrl).toString();
        if (newUrl === currentUrl) break;
        currentUrl = newUrl;
      } else {
        try {
          await Promise.all([
            page.waitForNavigation({ timeout: 60000 }),
            nextLink.click()
          ]);
          currentUrl = page.url();
        } catch (e) {
          console.error('Navigation failed:', e);
          break;
        }
      }

      pageNum++;
      if (pageNum > 50) break; // Limit to 50 pages
    }
  } finally {
    await browser.close();
  }

  // Transform the results to match the expected format
  return results.map(result => ({
    name: result.metadata.name || '',
    phone: result.metadata.phone || '',
    email: result.metadata.email || '',
    website: result.metadata.website || '',
    address: result.metadata.address || '',
    category: result.metadata.category || '',
    text: result.text,
    metadata: result.metadata
  }));
}

module.exports = {
  playwrightScraper
};

---

_**Assistant**_

To get more relevant Copilot Chat results, we need permission to read the contents of your repository on GitHub.

---

_**User**_

@workspace Grant: "Permission Request"

---

_**Assistant**_

Hereâs a focused comparison and explanation of why your columns (`lien`, `secteur`, `pays`, `source`, `site_web`, `email`, `telephone`, `adresse`) are no longer being populated in the new code in `scraperService.js`:

---

### **Key Differences**

#### 1. **Return Format of `playwrightScraper`**

**Old (working) code:**
- The `playwrightScraper` returned an array of objects with **flattened fields**:
  - `name`, `phone`, `email`, `website`, `address`, `category`, `text`, `metadata`
- This made it easy for the service to map these fields directly to your DB columns.

**New code (current `playwrightScraper.js`):**
- Returns an array of objects like:
  - `{ text: ..., metadata: { ... } }`
- The `metadata` object only contains keys that were found by the selectors. If a selector is missing or not matched, the field is missing or empty.

#### 2. **Mapping in `scraperService.js`**

**Old code:**
- The mapping logic in `executeScraper` used both the top-level fields and `metadata` to populate the DB columns.
- It also used regex extraction from the `content` string as a fallback.

**New code:**
- The mapping logic tries to use `result.name`, `result.phone`, etc., but the new `playwrightScraper` does **not** return these fields at the top level.
- Instead, all data is inside `result.metadata`, and if the selectors are not set up to extract e.g. `name`, `category`, etc., those fields are missing.
- The fallback regex extraction is only used if the metadata is missing, but if the selectors are not extracting the right fields, the columns remain empty.

#### 3. **Selectors and Extraction**

- If your `selectors.child` does **not** include keys like `name`, `category`, etc., those fields will never be populated in `metadata`.
- The old code was more forgiving, using regex on the whole text to try to extract values if selectors failed.

---

### **Why Your Columns Are Empty**

- **Your new `playwrightScraper` only puts what you explicitly extract via selectors into `metadata`.**
- If your selectors do not include `name`, `secteur`, `pays`, etc., or if the selectors are incorrect, those fields are missing.
- The mapping in `scraperService.js` expects these fields to be present in `metadata` or at the top level, but they are not.

---

### **How to Fix**

#### **Option 1: Add Fallback Extraction (like old code)**
Update the mapping in `scraperService.js` to use regex extraction from `result.text` if the metadata fields are missing.

#### **Option 2: Ensure Selectors Extract All Needed Fields**
Make sure your `selectors.child` includes all the keys you want to populate (`name`, `secteur`, `pays`, etc.), and that the selectors are correct.

---

### **Example Fix for Mapping**

Hereâs how you can update the mapping logic to restore the old behavior:

````javascript
// ...existing code...
scrapedData = scrapedData.map((result, index) => {
  const existing = safeExistingData[index] || {};
  const existingMetadata = existing.metadata || {};
  const text = result.text || '';
  // Use metadata if available, otherwise fallback to regex extraction
  const newMetadata = {
    name: result.metadata?.name || extractInfo(text, /(?:name|nom)\s*:?\s*([^\n]+)/i) || existingMetadata.name,
    phone: result.metadata?.phone || extractInfo(text, /(?:tel|phone|tÃ©lÃ©phone)\s*:?\s*([^\n]+)/i) || existingMetadata.phone,
    email: result.metadata?.email || extractInfo(text, /(?:email|e-mail|mail)\s*:?\s*([^\n]+)/i) || existingMetadata.email,
    website: result.metadata?.website || extractInfo(text, /(?:website|site web|site)\s*:?\s*([^\n]+)/i) || existingMetadata.website,
    address: result.metadata?.address || extractInfo(text, /(?:address|adresse)\s*:?\s*([^\n]+)/i) || existingMetadata.address,
    category: result.metadata?.category || extractInfo(text, /(?:sector|secteur)\s*:?\s*([^\n]+)/i) || existingMetadata.category
  };
  return {
    title: newMetadata.name || scraper.name,
    content: result.text,
    url: scraper.source || scraper.url,
    metadata: newMetadata
  };
});
// ...existing code...
````

---

### **Summary Table**

| Old Code Behavior | New Code Behavior | Solution |
|-------------------|------------------|----------|
| Extracted fields directly or via regex fallback | Only extracts fields present in selectors.child | Add regex fallback or fix selectors |

---

**In short:**  
Your new code is stricter and only populates columns if the selectors extract those fields. To restore the old behavior, add regex fallback extraction in the mapping step, or ensure your selectors always extract all needed fields.

- `scraperService.js`
- `playwrightScraper.js`

Let me know if you want the exact patch for your current code!

---

